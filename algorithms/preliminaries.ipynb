{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dlab project: Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arthurchansel/anaconda3/envs/mnlp/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Users/arthurchansel/anaconda3/envs/mnlp/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "Write a story about a data scientist.<|im_end|>\n",
      "\n",
      "<|im_start|>user\n",
      "Write a story about a data scientist.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "**The Data Catastrophe**\n",
      "\n",
      "Ramsaura, the young woman without a surname, was assigned to the prestigious Data Science department at the prestigious Harvard Business School. She was tasked with analyzing an enormous dataset of pet purchases\n"
     ]
    }
   ],
   "source": [
    "# pip install transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "checkpoint = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"Write a story about a data scientist.\"}]\n",
    "input_text=tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(input_text)\n",
    "inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "n = 50\n",
    "outputs = model.generate(inputs, max_new_tokens=n, do_sample=True, return_dict_in_generate=True, output_scores=True)\n",
    "print(tokenizer.decode(outputs['sequences'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihood: 1.3217267224320587e-13\n",
      "Geometric mean: 0.5526152090933232\n"
     ]
    }
   ],
   "source": [
    "# likelihood, and geometric mean\n",
    "\n",
    "likelihood = 1\n",
    "for logits in outputs['scores']:\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    likelihood *= float(torch.max(probs))\n",
    "print('Likelihood:', likelihood)\n",
    "print('Geometric mean:', likelihood**(1/n))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Product of likelihoods: for 50 tokens we could expect something between 1e-30 and 1e-100, here we have 4e-9, which tells our sequence might be very likely.\n",
    "\n",
    "Geometric mean: vary between 0 and 1 (less to most likely, respectively), here we have 0.7, which again confirms how likely our sequence is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. mask special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more special tokens might exist, search for them\n",
    "\n",
    "from transformers import LogitsProcessorList, LogitsProcessor\n",
    "class BlacklistTokensProcessor(LogitsProcessor):\n",
    "    def __init__(self, blacklist_token_ids):\n",
    "        self.blacklist_token_ids = blacklist_token_ids\n",
    "\n",
    "    def __call__(self, input_ids, scores):\n",
    "        # Set the logits of blacklisted tokens to a very large negative value (effectively 0 probability)\n",
    "        for token_id in self.blacklist_token_ids:\n",
    "            scores[:, token_id] = -float('inf')\n",
    "        return scores\n",
    "\n",
    "blacklist_tokens = [\".\", \"!\", \"?\", \",\", \";\", \":\", \"*\", \"**\",\n",
    "                    \"...\", \"(\", \")\", \"-\", \"\\\"\", \",\\\"\", \"Ġ(\",\n",
    "                    \").\", \"Ġ-\", \"ĊĊ\", \"**:\", \"Ġ**\", \"!**\",\n",
    "                    \"<|im_end|>\", \".\\\"\", \"\\\":\", \"---\", \"..\",\n",
    "                    \"<|endoftext|>\", \"ĠâĢĵ\"]\n",
    "blacklist_token_ids = tokenizer.convert_tokens_to_ids(blacklist_tokens)\n",
    "\n",
    "logits_processor = LogitsProcessorList()\n",
    "logits_processor.append(BlacklistTokensProcessor(blacklist_token_ids))\n",
    "\n",
    "n=100\n",
    "outputs = model.generate(inputs, max_new_tokens=n, do_sample=True, return_dict_in_generate=True, output_scores=True, logits_processor=logits_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "Write a story about a data scientist.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Title\n",
      "Story 2\n",
      "\n",
      "It was a typical Monday morning at the corporate firm where the CEO of Tech Industries was leading a team of data scientists to investigate a new project of his that had been stalled for months due to the sheer volume of data generating around it all the time over the past year or two of progress made on the project's timeline was a losing battle for the tech industry as a whole with the sheer volume of data being generated continuously and the sheer level of noise\n",
      "\n",
      "## TOKENS ANALYSIS:\n",
      "Output: \n",
      "\n",
      "TokenID: 198\n",
      "Token: Ċ\n"
     ]
    }
   ],
   "source": [
    "output_text = tokenizer.decode(outputs['sequences'][0])\n",
    "print(output_text)\n",
    "\n",
    "print('\\n## TOKENS ANALYSIS:')\n",
    "token_pos = 16\n",
    "print('Output:',tokenizer.decode(outputs['sequences'][0][token_pos]))\n",
    "token_id = outputs['sequences'][0][token_pos].item()\n",
    "print('TokenID:',token_id)\n",
    "print('Token:',tokenizer.convert_ids_to_tokens(token_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. beamsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask generations of forbidden tokens\n",
    "\n",
    "n=50\n",
    "outputs = model.generate(\n",
    "    inputs,\n",
    "    max_new_tokens=n,\n",
    "    do_sample=True,\n",
    "    logits_processor=logits_processor,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    num_beams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "Write a story about a data scientist.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Meet Maya Singh\n",
      "\n",
      "Maya Singh was a data scientist at a leading tech firm in Silicon Valley who had been working on a project to develop a predictive model to forecast the sales of a new product launch in the city's coffee shops\n"
     ]
    }
   ],
   "source": [
    "output_text = tokenizer.decode(outputs['sequences'][0])\n",
    "print(output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
